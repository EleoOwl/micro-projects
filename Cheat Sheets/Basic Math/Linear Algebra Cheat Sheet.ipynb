{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Linear Algebra Cheat Sheet</h2></center>\n",
    "\n",
    "* [Systems of vectors](#Systems-of-vectors)\n",
    "* [Linear operators](#Linear-operators)\n",
    "* [Anatomy of matrix multiplication](#Anatomy-of-matrix-multiplication)\n",
    "* [Matrix representations of linear operators](#Matrix-representations-of-linear-operators)\n",
    "* [Change of basis](#Change-of-basis)\n",
    "* [Inner product and norm](#Inner-product-and-norm)\n",
    "* [Orthogonality](#Orthogonality)\n",
    "* [Fundamental Subspaces](#Fundamental-Subspaces)\n",
    "* [Eigenvalues and eigenvectors](#Eigenvalues-and-eigenvectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3>Systems of vectors</h3></center>\n",
    "\n",
    "Let $V$ be vector space over field $F$. It is enough to have intuitive understanding of these terms. If you want formal definitions and examples - check Wikipedia.\n",
    "\n",
    "Elements of $V$ are usually called **vectors** or **points** (the former being more abstract term and the latter assisting the geometric intuition). Elements of $F$ are called **scalars**. We'll deal only with the case when $F = \\mathbb{R}$ - field of real numbers.\n",
    "\n",
    "The sequence $v_1, \\dots, v_n$ of vectors of $V$ is called **system of vectors** or sometimes even **ordered system of vectors**. We'll deal only with finite systems of vectors but in general they can be infinite and even uncountable.\n",
    "\n",
    "<br>\n",
    "\n",
    "Vector $w$ **can be represented as linear combination** of vectors $v_1, \\dots, v_n \\in V$ iff there exists *at least one* sequence of scalars $\\alpha_1, \\dots, \\alpha_n \\in F$ such that $w = \\alpha_1 v_1 + \\dots \\alpha_n v_n$. We use the term **linear combination with coefficients $\\alpha_1, \\dots, \\alpha_n$** to speak about any particular such representation.\n",
    "\n",
    "The set of all possible linear combinations of vectors $v_1, \\dots, v_n$ (i.e. the set of all vectors that can be represented as linear combination of $v_1, \\dots, v_n$) is called **linear span** of $v_1, \\dots, v_n$. It is sometimes denoted as $\\langle v_1, \\dots, v_n \\rangle$:\n",
    "$$\n",
    "\\langle v_1, \\dots, v_n \\rangle = \n",
    "\\{\n",
    "\\alpha_1 v_1 + \\dots + \\alpha_n v_n\n",
    "|\n",
    "\\alpha_1, \\dots, \\alpha_n \\in F\n",
    "\\}\n",
    "$$\n",
    "\n",
    "If linear span of $v_1, \\dots, v_n$ coincides with $V$ (i.e. if any vector in $V$ can be represented as their linear combination) then we say that $V$ is **spanning system** of $V$.\n",
    "\n",
    "The linear span is always a subspace of $V$ i.e.\n",
    "$$\n",
    "\\forall k \\in \\mathbb{N} ~~~\n",
    "\\forall \\alpha_1, \\dots, \\alpha_k \\in F ~~~\n",
    "\\forall x_1, \\dots, x_k \\in \\langle v_1, \\dots, v_n \\rangle ~~~~\n",
    "\\alpha_1 x_1 + \\dots + \\alpha_k x_k \\in \\langle v_1, \\dots, v_n \\rangle\n",
    "$$\n",
    "\n",
    "In particular zero vector always belongs to $ \\langle v_1, \\dots, v_n $.\n",
    "\n",
    "<br>\n",
    "\n",
    "We call system $v_1, \\dots, v_n$ **linearly independent** iff the following equivalent conditions hold:\n",
    "* *any* vector in $\\langle v_1, \\dots, v_n \\rangle$ has unique  representation:\n",
    "<center>\n",
    "if $\\alpha_1 v_1 + \\dots + \\alpha_n v_n = \\beta_1 v_1 + \\dots + \\beta_n v_n$ then $\\alpha_1 = \\beta_1, \\dots, \\alpha_n = \\beta_n$\n",
    "</center>\n",
    "* there exists *at least one* vector $w \\in \\langle v_1, \\dots, v_n$ that has unique representation:\n",
    "<center>\n",
    "if $w = \\alpha_1 v_1 + \\dots + \\alpha_n v_n$ and $w = \\beta_1 v_1 + \\dots + \\beta_n v_n$ then $\\alpha_1 = \\beta_1, \\dots, \\alpha_n = \\beta_n$\n",
    "</center>\n",
    "* $0$ has unique representation:\n",
    "<center>\n",
    "if $\\alpha_1 v_1 + \\dots \\alpha_n v_n = 0$ then $\\alpha_1 = \\dots = \\alpha_n = 0$\n",
    "</center>\n",
    "(be cautious: the first zero is vector, the second one is scalar)\n",
    "\n",
    "We call system $v_1, \\dots, v_n$ **linearly dependent** if it is not linearly independent (i.e. at least one vector has two different representations as linear combination of the system's vectors).\n",
    "\n",
    "Vector space is called **finite-dimensional** iff all of its linearly independent systems are finite. Otherwise it's called **infinite-dimensional**. We'll deal only with finite-dimensional spaces.\n",
    "\n",
    "<br>\n",
    "\n",
    "We call system $v_1, \\dots, v_n$ a **subsystem** of $w_1, \\dots, w_m$ iff there exists an increasing sequence of indexes $i_1 < \\dots < i_n \\leq m$ such that $w_{i_k} = v_{k}$. In this case we also call $w_1, \\dots, wm$ a **supersystem** of $v_1, \\dots, v_n$.\n",
    "\n",
    "Let $P$ be any property of systems of vectors (for example \"being linearly independent\").\n",
    "\n",
    "We call system $v_1, \\dots, v_n$ **minimal system with property $P$** iff none of its subsystems has property $P$.\n",
    "\n",
    "We call system $v_1, \\dots, v_n$ **maximal system with property $P$** iff none of its supersystems has property $P$.\n",
    "\n",
    "We call system $v_1, \\dots, v_n$ a **basis** of $V$ iff the following equivalent conditions hold:\n",
    "* it is the minimal spanning system in $V$;\n",
    "* it is the maximal linearly independent system in $V$;\n",
    "* any vector $w \\in V$ has unique representation as linear combination of $v_1, \\dots, v_n$.\n",
    "\n",
    "It can be proved that any vector space has a basis and that all bases of finite-dimensional vector space have the same number of vectors. This number is called the **dimension** of vector space. Any subspace of finite-dimensional space is also finite-dimensional and it dimension is not greater than dimension of the \"main space\".\n",
    "\n",
    "Vector space $\\mathbb{R}^n$ has dimension $n$ and its **standard basis** is defined to be\n",
    "$$\n",
    "e_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}, ~~~\n",
    "e_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ \\vdots \\\\ 0 \\end{bmatrix}, ~~~\n",
    "\\dots, ~~~\n",
    "e_n = \\begin{bmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ n \\end{bmatrix}, ~~~\n",
    "$$\n",
    "\n",
    "**Rank** of the system $v_1, \\dots, v_n$ is the maximum number of vectors in its linearly independent subsystems. I.e. if rank is $r$ than $v_1, \\dots, v_n$ has at least one linearly independent subsystem with $r$ vectors but all its subsystem having more vectors are linearly dependent. System's rank coincides with the dimension of its linear span. Rank of any subsystem is always not greater than the rank of the \"main system\".\n",
    "\n",
    "**Extension property of linearly independent systems**. Let $V$ be $n$-dimensional vector space and $v_1, \\dots, v_m \\in V$ be linearly independent system. Then $m \\leq n$ and if $m < n$ then there exist vector $w_{m+1}, \\dots, w_n$ such that $v_1, \\dots, v_m, w_{m+1}, \\dots, w_n$ is basis of $V$.\n",
    "\n",
    "If $x \\in V$ and $e_1, \\dots, e_n$ is some basis of $V$ then $x$ has unique representation $x = \\xi_1 e_1 + \\dots + \\xi_n e_n$. We call column vector\n",
    "$ [x]_e = \\begin{bmatrix} \\xi_1 \\\\ \\vdots \\\\ \\xi_n \\end{bmatrix} $ a **coordinate vector** of $x$ with respect to basis $e$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3>Linear operators</h3></center>\n",
    "\n",
    "[top](#Linear-Algebra-Cheat-Sheet)\n",
    "\n",
    "Let $V_1$ and $V_2$ be vector spaces over the same field $F$. Mapping $\\mathcal{A}: V_1 \\rightarrow V_2$ is called **linear operator** (or **homomorphism** of vector spaces) iff it is \"structure-preserving\":\n",
    "$$\n",
    "\\forall \\alpha, \\beta \\in F ~~~\n",
    "\\forall x, y \\in V_1 ~~~\n",
    "\\mathcal{A}(\\alpha x + \\beta y) =\n",
    "\\alpha \\mathcal{A}(x) + \\beta \\mathcal{A}(y)\n",
    "$$\n",
    "\n",
    "Two nice properties of linear operators:\n",
    "* if $L_1$ is a subspace of $V_1$ then its image $\\{\\mathcal{A}x | x \\in L_1 \\}$ is a subspace of $V_1$;\n",
    "* if $L_2$ is a subspace of $V_2$ then its preimage $\\{x \\in V_1 | \\mathcal{A}(x) \\in L_2 \\}$ is a subspace of $V_2$.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Image** of $\\mathcal{A}$ is the set $\\text{im} \\mathcal{A} = \\{ \\mathcal{A}x ~|~ x \\in L_1 \\} \\subset V_2$. Image is always a subspace of $V_2$. If $V_1$ is finite-dimensional then $\\text{im}$ is also finite-dimensional and its dimension is called **rank** of $\\mathcal{A}$.\n",
    "\n",
    "**Kernel** of $\\mathcal{A}$ is the set $\\ker \\mathcal{A} = \\{ x \\in L_1 | \\mathcal{A}x = 0 \\}$. Kernel is always a subspace of $V_1$. If $V_1$ is finite-dimensional then $\\ker \\mathcal{A}$ is also finite-dimensional and its dimension is called **nullity** of $\\mathcal{A}$.\n",
    "\n",
    "**Rank-nullity theorem**. If $V_1$ is finite-dimensional then\n",
    "$$ \\dim \\text{im} \\mathcal{A} + \\dim \\ker \\mathcal{A} = \\dim V_1 $$\n",
    "\n",
    "<br>\n",
    "\n",
    "Linear operator is **one-to-one** (or **injection**) iff for any vectors $x, y \\in V_1$ equality $\\mathcal{A}x = \\mathcal{A}y$ implies equality $x = y$. If the operator is one-to-one then we can define **inverse operator**\n",
    "$$ \\mathcal{A}^{-1}: \\text{im}\\mathcal{A} \\rightarrow V_1 $$\n",
    "with the property\n",
    "$$\n",
    "\\forall x \\in V_1 ~~ \\mathcal{A}^{-1}(\\mathcal{A} x) = x, ~~~\n",
    "\\forall y \\in \\text{im}\\mathcal{A} ~~ \\mathcal{A}(\\mathcal{A}^{-1}y) = y\n",
    "$$\n",
    "It can be shown that\n",
    "* $\\mathcal{A}$ is one-to-one iff $\\ker\\mathcal{A} = {0}$;\n",
    "* $\\mathcal{A}$ is one-to-one iff it preserves linear independence: if for any linearly independent system $v_1, \\dots, v_n \\in V_1$ their images $\\mathcal{A}v_1, \\dots, \\mathcal{A}v_n$ are also linearly independent;\n",
    "* $\\mathcal{A}^{-1}$ is also a linear operator.\n",
    "\n",
    "Linear operator is a **surjection** iff $\\text{im}\\mathcal{A} = V_2$ i.e. iff any vector in $V_2$ is an image of at least on vector in $V_1$:\n",
    "$$ \\forall y \\in V_2 ~~ \\exists x \\in V_1 ~~~~ y = \\mathcal{A}x $$\n",
    "\n",
    "Mapping $\\mathcal{A}: V_1 \\rightarrow V_2$ is an **isomorphism** of vector spaces iff it's a bijective linear operator (\"bijective\" means being injective and surjective simultaneously). If there exists isomorphism between two vector spaces then they are called **isomorphic**. These means that they have the same structure.\n",
    "\n",
    "Let $V_1$ and $V_2$ be finite-dimensional vector spaces. There exists isomorphism $\\mathcal{A}: V_1 \\rightarrow V_2$ iff $\\dim V_1 = \\dim V_2$. Moreover if $\\dim V_1 = \\dim V_2$ then the following conditions are equivalent:\n",
    "* $\\mathcal{A}$ is bijective;\n",
    "* $\\mathcal{A}$ is injective;\n",
    "* $\\mathcal{A}$ is surjective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3>Anatomy of matrix multiplication</h3></center>\n",
    "\n",
    "[top](#Linear-Algebra-Cheat-Sheet)\n",
    "\n",
    "I assume that you know how to add, multiply and transpose matrices and basic properties of these operations. The most important thing to understand is \"anatomy\" of multiplication.\n",
    "\n",
    "Let $A \\in \\mathbb{R}^{m \\times n}$. Let's denote its $i$-th row by $A_{i \\bullet}$ and its $j$-th column by $A_{\\bullet j}$.\n",
    "\n",
    "If $x \\in \\mathbb{R}^n$ then $A x$ is column vector such that\n",
    "$$ A x = x_1 A_{\\bullet 1} + \\dots + x_n A_{\\bullet n} $$\n",
    "\n",
    "If $x \\in \\mathbb{R}^m$ then $x^T A$ is row vector such that\n",
    "$$ x^T A = x_1 A_{1 \\bullet} + \\dots + x_m A_{m \\bullet} $$\n",
    "\n",
    "Let $B \\in \\mathbb{R}^{n \\times p}$. Then\n",
    "$$ (AB)_{ij} = \\sum_{k=1}^n A_ik B_kj = A_{i \\bullet} B_{\\bullet j} $$\n",
    "$$\n",
    "(AB)_{i \\bullet} = A_{i \\bullet} B =\n",
    "A_{i1} B_{1 \\bullet} + \\dots + A_{in} B_{n \\bullet}\n",
    "$$\n",
    "$$ \n",
    "(AB)_{\\bullet j} = A B_{\\bullet j} =\n",
    "B_{1j} A_{\\bullet 1} + \\dots + B_{nj} A_{\\bullet n}\n",
    "$$\n",
    "\n",
    "Matrix multiplication is non-commutative in general. For product $AB$ and $BA$ to be both well-defined and possibly equal we need to restrict ourselves to square matrices. The only matrices that commute with any other matrices are scalar multiples of identity matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3>Matrix representations of linear operators</h3></center>\n",
    "\n",
    "[top](#Linear-Algebra-Cheat-Sheet)\n",
    "\n",
    "Let $\\dim V_1 = n > 0$ and let $e_1, \\dots, e_n$ be arbitrary basis of $V_1$. If $x \\in V_1$ then $x$ has unique representation $x = \\xi_1 e_1 + \\dots + \\xi_n e_n$ and\n",
    "$$ \\mathcal{A}(x) = \\xi_1 \\mathcal{A}e_1 + \\dots + \\xi_n \\mathcal{A}e_n $$\n",
    "\n",
    "Let $\\dim V_2 = m > 0$ and $f_1, \\dots, f_m$ be arbitrary basis of $V_2$. Let $A \\in \\mathbb{R}^{m \\times n}$ be matrix whose $j$-th column is coordinate vector of $\\mathcal{A}e_j$ with respect to basis $f$:\n",
    "$$\n",
    "A =\n",
    "\\begin{bmatrix}\n",
    "| & \\dots & | \\\\\n",
    "[\\mathcal{A}e_1]_f & \\dots & [\\mathcal{A}e_n]_f \\\\\n",
    "| & \\dots & | \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then\n",
    "$$ [\\mathcal{A} x]_f = A [x]_e $$\n",
    "\n",
    "We'll call matrix $A$ **matrix representation** of operator $\\mathcal{A}$ with respect to bases $e$ and $f$.\n",
    "\n",
    "<br>\n",
    "\n",
    "Any matrix $A \\in \\mathbb{R}^{m \\times n}$ can be vied as matrix representation with respect to standard bases of linear operator\n",
    "$$ \\mathcal{A}: \\mathbb{R}^n \\ni x ~ \\rightarrow ~ Ax \\in \\mathbb{R}^m $$\n",
    "\n",
    "We'll call this operator a **standard operator** of matrix $A$.\n",
    "\n",
    "The space of all linear operators acting from $\\mathbb{R}^n$ to $\\mathbb{R}^m$ and the space of all $m \\times n$ real matrices are isomorphic. Matrix representation for sum/composition of linear operators is sum/product of their individual matrix representations.\n",
    "\n",
    "If $\\mathcal{A}: V \\rightarrow W$ is a bijective linear operator with matrix representation $A$ then matrix representation of $\\mathcal{A}^{-1}$ with respect to the same pair of bases is $A^{-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3>Change of basis</h3></center>\n",
    "\n",
    "[top](#Linear-Algebra-Cheat-Sheet)\n",
    "\n",
    "Let $e_1, \\dots, e_n$ and $f_1, \\dots, f_n$ be two bases for vector space $V$ and let $\\mathcal{I}: V \\ni x ~ \\rightarrow ~ x \\in V$ be identity operator with matrix representation $T_{e \\rightarrow f}$ with respect to bases $e$ and $f$ i.e.\n",
    "$(T_{e \\rightarrow f})_{\\bullet j} = [\\mathcal{I}e_j]_f = [e_j]_f$. Then\n",
    "$$\n",
    "\\forall x \\in V ~~~\n",
    "[x]_f = [\\mathcal{I}x]_f = T_{e \\rightarrow f} [x]_e\n",
    "$$\n",
    "This is the **first change of basis formula**.\n",
    "\n",
    "<br>\n",
    "\n",
    "Let now $e_1, \\dots, e_n$ and $E_1, \\dots, E_n$ be two bases on vector space $V_1$; $f_1, \\dots, f_m$ and $F_1, \\dots, F_M$ be two bases on vector space $V_2$. Let\n",
    "* $A_{e-f}$ be matrix representation of linear operator $\\mathcal{A}: V_1 \\rightarrow V_2$ with respect to bases $e$ and $f$.\n",
    "* $T_{e-E}$ be matrix representation of identity operator $\\mathcal{I_1}: V_1 \\ni x ~ \\rightarrow x \\in V_1$ with respect to bases $e$ and $E$.\n",
    "* $T_{f-F}$ be matrix representation of identity operator $\\mathcal{I_2}: V_2 \\ni x ~ \\rightarrow x \\in V_2$ with respect to bases $f$ and $F$.\n",
    "\n",
    "Operator equality\n",
    "$ \\mathcal{A} = \\mathcal{I_2} \\mathcal{A} \\mathcal{I_1} $\n",
    "implies matrix equality\n",
    "$$\n",
    "A_{E-F} = I_{f-F} A_{e-f} I_{E-e}\n",
    "$$\n",
    "\n",
    "This is the **second change of basis formula**.\n",
    "\n",
    "Let's consider a particular case when $V_1 = V_2$ and $e = f$, $E = F$. In this case the formula looks like\n",
    "$$\n",
    "A_{E-E} = I_{e-E} A_{e-e} I_{E-e} = \n",
    "(I_{E-e})^{-1} A_{e-e} I_{E-e} = I_{e-E} A_{e-e} (I_{e-E})^{-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3>Inner product and norm</h3></center>\n",
    "\n",
    "[top](#Linear-Algebra-Cheat-Sheet)\n",
    "\n",
    "Let $V$ be vector space over $\\mathbb{R}$. **Inner product** on $V$ is function $\\langle \\bullet, \\bullet \\rangle: ~ V \\times V ~ \\rightarrow ~ \\mathbb{R}$ satsifying three properties:\n",
    "* symmetry:\n",
    "$$ ~\\langle x, y \\rangle = \\langle y, x \\rangle $$\n",
    "* linearity in the first argument:\n",
    "$$ ~\\forall \\alpha, \\beta \\in \\mathbb{R} ~~ \\forall x, y \\in V ~~ \\langle \\alpha x + \\beta y, z \\rangle = \\alpha \\langle x, z \\rangle + \\beta \\langle y, z \\rangle\n",
    "$$\n",
    "* positive-definiteness:\n",
    "    * $\\forall x \\in V ~~ \\langle x, x \\rangle \\geq 0$;\n",
    "    * $\\forall x \\in V ~~ \\langle x, x \\rangle = 0 ~ \\Leftrightarrow ~ x = 0$.\n",
    "\n",
    "Let $v_1, \\dots, v_n$ be basis of $V$ and $x = x_1 v_1 + \\dots x_n v_n$, $y = y_1 v_1 + \\dots + y_n v_n$. Then\n",
    "$$\n",
    "\\langle x, y \\rangle =\n",
    "\\sum_{i=1}^{n} \\sum_{j=1}^{n} x_i y_j \\langle e_i, e_j \\rangle\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "**Norm** on $V$ is function $\\langle \\bullet, \\bullet \\rangle: ~ V \\times V ~ \\rightarrow ~ \\mathbb{R}$ satsifying three properties:\n",
    "* triangle inequality: $ \\forall x, y \\in V ~~ \\Vert x + y \\Vert \\leq \\Vert x \\Vert + \\Vert y \\Vert $;\n",
    "* absolute homogenity: $ \\forall \\alpha \\in F ~~ \\forall x \\in V ~~ \\Vert \\alpha x \\Vert = |\\alpha| \\Vert x \\Vert $;\n",
    "* positive-definiteness:\n",
    "    * $\\forall x \\in V ~~ \\Vert x \\Vert \\geq 0$;\n",
    "    * $\\forall x \\in V ~~ \\Vert x \\Vert = 0 ~ \\Leftrightarrow ~ x = 0$.\n",
    "\n",
    "<br>\n",
    "\n",
    "If $V$ is vector space with inner product $\\langle \\bullet, \\bullet \\rangle$ then function $ V \\ni x ~ \\rightarrow \\sqrt{\\langle x, x \\rangle} ~~ \\in \\mathbb{R}_{\\geq 0} $ defines norm on $V$. This norm is said to be **generated by inner product**.\n",
    "\n",
    "**Cauchy–Bunyakovsky–Schwarz inequality**.\n",
    "$$\n",
    "\\forall x, y \\in V ~~~ | \\langle x, y \\rangle |^2 \\leq\n",
    "\\langle x, x \\rangle \\cdot \\langle y, y \\rangle\n",
    "$$\n",
    "$$\n",
    "\\forall x, y \\in V ~~~ | \\langle x, y \\rangle | \\leq\n",
    "\\Vert x \\Vert \\cdot \\Vert y \\Vert\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3>Orthogonality</h3></center>\n",
    "\n",
    "[top](#Linear-Algebra-Cheat-Sheet)\n",
    "\n",
    "We say that vector $x$ is **orthogonal** to vector $y$ iff $\\langle x, y \\rangle = 0$. We denote this by $x \\perp y$.\n",
    "* if $x \\perp y$ then $y \\perp x$;\n",
    "* if $\\forall x \\in V ~~ x \\perp y$ then $y = 0$.\n",
    "\n",
    "Let $A$ be nonempty subset of $V$ and $x \\in V$. We say that vector $x$ is orthogonal to $A$ iff $\\forall y \\in A ~~ x \\perp y$. We denote this by $x \\perp A$.\n",
    "\n",
    "**Orthogonal complement** of set $A$ is the set of all vectors in $V$ orthogonal to $A$:\n",
    "$$\n",
    "A^\\perp = \\{ x \\in V ~|~ x \\perp A \\} =\n",
    "\\{x \\in V ~|~ \\forall y \\in A ~~ x \\perp y \\}\n",
    "$$\n",
    "\n",
    "Orthogonal complement is always a subspace.\n",
    "\n",
    "If $L$ is finite-dimensional subspace then\n",
    "* $\\left( L^\\perp \\right)^\\perp = L$;\n",
    "* $V = L \\oplus L^\\perp$ (this notation means that any vector $x \\in V$ has unique representation of the form $x = x' + x''$ where $x' \\in L$ and $x'' \\in L^\\perp$.\n",
    "\n",
    "<br>\n",
    "\n",
    "System $v_1, \\dots, v_n$ is **orthogonal** iff all $v_i$ are nonzero and pairwise orthogonal: $v_i \\perp v_j ~~ (i \\neq j)$.\n",
    "\n",
    "System $v_1, \\dots, v_n$ is **orthonormal** iff all $v_i$ are pairwise orthogonal and have unit norm:\n",
    "$$\n",
    "\\langle v_i, v_j \\rangle = \n",
    "\\begin{cases}\n",
    "0 & i \\neq j \\\\\n",
    "1 & i = j\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Basis is called orthogonal/orthonormal iff its vectors form orthogonal/orthonormal system.\n",
    "\n",
    "If $e_1, \\dots, e_n$ is orthogonal basis of and $x = x_1 v_1 + \\dots + x_n v_n$ then\n",
    "$$ x_i = \\frac{\\langle x, e_i \\rangle}{\\langle e_i, e_i \\rangle} $$\n",
    "\n",
    "If $e_1, \\dots, e_n$ is orthonormal basis and $x = x_1 v_1 + \\dots + x_n v_n$ then\n",
    "$$ x_i = \\langle x, e_i \\rangle $$\n",
    "\n",
    "<br>\n",
    "\n",
    "Let $A$ be nonempty subset of $V$ and $x \\in V$. We call vector $x' \\in A$ a **closest element** to $x$ in $A$ iff\n",
    "$$ \\forall y \\in A ~~~~ \\Vert x - x' \\Vert \\leq \\Vert x - y \\Vert $$\n",
    "or equivalently iff\n",
    "$$ \\Vert x - x' \\Vert = \\inf_{y \\in V} \\Vert x - y \\Vert $$\n",
    "\n",
    "**Closest element theorem**. Let $L$ be finite-dimensional subspace of $V$ and $x \\in V$. Then\n",
    "* there exists unique closest element to $x$ in $A$;\n",
    "* $y \\in V$ is the closest element iff $(x - y) \\perp L $."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3>Fundamental Subspaces</h3></center>\n",
    "\n",
    "[top](#Linear-Algebra-Cheat-Sheet)\n",
    "\n",
    "Let $A \\in \\mathbb{R}^{m \\times n}$. There are four **fundamental subspaces** asociated with matrix $A$.\n",
    "\n",
    "**Column space** is linear span of $A$'s columns:\n",
    "$$\n",
    "\\mathcal{C}(A) =\n",
    "\\text{span}(A_{\\bullet 1}, \\dots, A_{\\bullet n}) =\n",
    "\\{ x_1 A_{\\bullet 1} + \\dots x_n A_{\\bullet n} ~|~ x_1, \\dots, x_n \\in \\mathbb{R} \\} =\n",
    "\\{ Ax ~|~ x \\in \\mathbb{R}^n \\} \n",
    "$$\n",
    "\n",
    "**Nullspace** is defined as\n",
    "$$\n",
    "\\mathcal{N}(A) = \n",
    "\\{ x \\in \\mathbb{R}^n ~|~ Ax = 0 \\} \n",
    "$$\n",
    "\n",
    "**Row space** is column space of $A^T$ (so it is a linear span of transposed rows):\n",
    "$$\n",
    "\\mathcal{C}(A^T) =\n",
    "\\text{span} \\left( (A_{1 \\bullet})^T, \\dots, (A_{m \\bullet})^T \\right) =\n",
    "\\text{span}\\left( (A^{T})_{\\bullet 1}, \\dots, (A^{T})_{\\bullet m} \\right) =\n",
    "$$\n",
    "$$\n",
    "=\n",
    "\\left\\{ x_1 (A^T)_{\\bullet 1} + \\dots x_m (A^T)_{\\bullet m} ~|~ x_1, \\dots, x_n \\in \\mathbb{R} \\right\\} =\n",
    "\\{ A^T x ~|~ x \\in \\mathbb{R}^n \\} \n",
    "$$\n",
    "\n",
    "**Left Nullspace** is nullspace of $A^T$:\n",
    "$$\n",
    "\\mathcal{N}(A^T) = \n",
    "\\{ x \\in \\mathbb{R}^m ~|~ A^T x = 0 \\} \n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "* $\\mathcal{C}(A), \\mathcal{N}(A^T) \\subset \\mathbb{R}^m$;\n",
    "* $\\mathcal{N}(A), \\mathcal{C}(A^T) \\subset \\mathbb{R}^n$;\n",
    "* $\\mathcal{N}(A^T) = \\left( \\mathcal{C}(A) \\right)^\\perp$;\n",
    "* $\\mathcal{C}(A^T) = \\left( \\mathcal{N}(A) \\right)^\\perp$.\n",
    "\n",
    "<br>\n",
    "\n",
    "If $\\mathcal{A}$ is standard operator of matrix $A$ then\n",
    "* $ \\mathcal{C}(A) = \\text{im}(\\mathcal{A}) $;\n",
    "* $ \\mathcal{N}(A) = \\text{ker}(\\mathcal{A})$.\n",
    "\n",
    "The dimension of $\\mathcal{C}(A)$ is called **rank** of matrix $A$.\n",
    "\n",
    "**Rank-nullity theorem for matrices**. Let $A \\in \\mathbb{R}^{m \\times n}$ be matrix of rank $r$. Then\n",
    "* $\\dim \\mathcal{C}(A) = r$;\n",
    "* $\\dim \\mathcal{N}(A) = n - r$;\n",
    "* $\\dim \\mathcal{C}(A^T) = r$;\n",
    "* $\\dim \\mathcal{N}(A^T) = m - r$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3>Eigenvalues and eigenvectors</h3></center>\n",
    "\n",
    "[top](#Linear-Algebra-Cheat-Sheet)\n",
    "\n",
    "Let $V$ be vector space over field $F$ and $\\mathcal{A}: V \\rightarrow V$ linear operator. Let $\\alpha \\in F$ be scalar and $v \\in V$ be *nonzero* vector.\n",
    "\n",
    "We say that $(\\alpha, v)$ is **eigenpair** of operator $\\mathcal{A}$ iff $ \\mathcal{A} v = \\alpha v $. In this case we call $\\alpha$ an **eigenvalue** of $\\mathcal{A}$ and $v$ an **eigenvector** of $\\mathcal{A}$ associated with $\\alpha$.\n",
    "\n",
    "Denote\n",
    "$$\n",
    "E_\\alpha(\\mathcal{A}) =\n",
    "\\{ v \\in V ~|~ \\mathcal{A} v = \\alpha v \\} =\n",
    "\\{ v \\in V ~|~ (\\mathcal{A} - \\alpha \\mathcal{I})v \\} =\n",
    "\\ker (\\mathcal{A} - \\alpha \\mathcal{I})\n",
    "$$\n",
    "\n",
    "Obviously $E_\\alpha(\\mathcal{A})$ is always a subspace of $V$. $\\alpha$ is eigenvalue of $\\mathcal{A}$ iff $E_\\alpha(\\mathcal{A}) \\neq {0}$ i.e. iff $E_\\alpha(\\mathcal{A})$ contains at least one nonzero vector. In this case we call $E_\\alpha(\\mathcal{A})$ an **eigenspace** of $\\mathcal{A}$.\n",
    "\n",
    "Let $(\\alpha, v)$ be eigenpair of $\\mathcal{A}$. Then for any polynomial $p$ $(p(\\alpha), v)$ is eigenpair of operator $p(\\mathcal{A})$.\n",
    "\n",
    "<br>\n",
    "\n",
    "If $v_1, \\dots, v_m$ are eigenvectors associated with pairwise distinct eigenvalues $\\alpha_1, \\dots, \\alpha_m$ then $v_1, \\dots, v_m$ are linearly independent.\n",
    "\n",
    "<br>\n",
    "\n",
    "Let $V$ be finite-dimensional with $\\dim V = n > 0$. Basis $e_1, \\dots, e_n$ of space $V$ is called **eigenbasis** of operator $\\mathcal{A}$ iff $e_1, \\dots, e_n$ are eigenvectors of $\\mathcal{A}$. Operator is called **diagonalizable**.\n",
    "\n",
    "Let $e_1, \\dots, e_n$ be eigenbasis of $\\mathcal{A}$ and $A_{v-v}$ be matrix representation of $\\mathcal{A}$ with respect to this basis. Then $A_{v-v}$ is diagonal and its diagonal elements are eigenvalues of $\\mathcal{A}$.\n",
    "\n",
    "Eigenpairs of $A$ are defined as eigenpairs of $A$'s standard operator. \n",
    "Let $A \\in \\mathbb{R}^{m \\times n}$, $\\alpha \\in \\mathbb{R}$ and $v \\in \\mathbb{R}^n$. Then $(\\alpha, v)$ is eigenpair of $A$ iff $Av = \\alpha v$. Just as with linear operators matrix is called diagonalizable iff it has eigenbasis.\n",
    "\n",
    "Matrix is diagonalizable iff there exists invertible matrix $V$ and diagonal matrix $D$ such that\n",
    "* columns of $V$ are eigenvectors of $A$;\n",
    "* diagonal elements of $D$ are eigenvlues of $A$;\n",
    "* $ A = V D V^{-1}$.\n",
    "\n",
    "**Spectral theorem**. If $A$ is symmetric matrix then\n",
    "* all of its eigenvalues are real;\n",
    "* eigenvectors of $A$ corresponding to different eigenvalues are orthogonal;\n",
    "* there exists orthonormal basis of $A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml36",
   "language": "python",
   "name": "ml36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
